{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "from scipy.spatial import Delaunay\n",
    "from skimage.measure import label, regionprops\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- Compute Morphological Features ---\n",
    "def compute_morph(contour, box):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    pts = contour - np.array([x1, y1])\n",
    "    h, w = y2 - y1 + 1, x2 - x1 + 1\n",
    "    mask = np.zeros((h, w), dtype=np.uint8)\n",
    "    rr = pts[:, 1].astype(int)\n",
    "    cc = pts[:, 0].astype(int)\n",
    "    mask[rr, cc] = 1\n",
    "    lbl = label(mask)\n",
    "    props = regionprops(lbl)\n",
    "    if not props:\n",
    "        return {'area': 0, 'perimeter': 0, 'eccentricity': 0,\n",
    "                'solidity': 0, 'circularity': 0}\n",
    "    r = props[0]\n",
    "    return {\n",
    "        'area': r.area,\n",
    "        'perimeter': r.perimeter,\n",
    "        'eccentricity': r.eccentricity,\n",
    "        'solidity': r.solidity,\n",
    "        'circularity': 4 * np.pi * r.area / (r.perimeter**2 + 1e-6)\n",
    "    }\n",
    "\n",
    "# --- Generate NetworkX Graph from .dat ---\n",
    "def generate_graph_from_dat(dat_path):\n",
    "    data = joblib.load(dat_path)\n",
    "    nodes = []\n",
    "    for idx, nucleus in enumerate(data):  # Iterate over list indices\n",
    "        contour = np.array(nucleus['contour'])\n",
    "        centroid = tuple(nucleus['centroid'])\n",
    "        ntype = nucleus['type']\n",
    "        x_min, y_min = contour.min(axis=0)\n",
    "        x_max, y_max = contour.max(axis=0)\n",
    "        box = (x_min, y_min, x_max, y_max)\n",
    "        morph = compute_morph(contour, box)\n",
    "        nodes.append({'id': idx,  # Use index as node ID\n",
    "                      'centroid': centroid,\n",
    "                      'type': ntype,\n",
    "                      **morph})\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for node in nodes:\n",
    "        G.add_node(node['id'],\n",
    "                   x=node['centroid'][0],\n",
    "                   y=node['centroid'][1],\n",
    "                   type=node['type'],\n",
    "                   area=node['area'],\n",
    "                   perimeter=node['perimeter'],\n",
    "                   eccentricity=node['eccentricity'],\n",
    "                   solidity=node['solidity'],\n",
    "                   circularity=node['circularity'])\n",
    "    points = np.array([n['centroid'] for n in nodes])\n",
    "    if len(points) >= 3:\n",
    "        tri = Delaunay(points)\n",
    "        edges = set()\n",
    "        for simplex in tri.simplices:\n",
    "            for i in range(3):\n",
    "                u, v = sorted([simplex[i], simplex[(i+1) % 3]])\n",
    "                edges.add((u, v))\n",
    "        for u_idx, v_idx in edges:\n",
    "            p1, p2 = points[u_idx], points[v_idx]\n",
    "            dist = np.linalg.norm(p1 - p2)\n",
    "            inv_dist = 1.0 / (dist + 1e-6)\n",
    "            factor = 1.0 if nodes[u_idx]['type'] == nodes[v_idx]['type'] else 0.5\n",
    "            weight = inv_dist * factor\n",
    "            G.add_edge(nodes[u_idx]['id'], nodes[v_idx]['id'], weight=weight)\n",
    "    return G\n",
    "\n",
    "# --- Convert NetworkX Graph to PyG Data ---\n",
    "def convert_nx_to_pyg(G, label):\n",
    "    # Convert node labels to integers\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    \n",
    "    # Extract node features\n",
    "    node_features = []\n",
    "    feature_names = ['x', 'y', 'type', 'area', 'perimeter', 'eccentricity', 'solidity', 'circularity']\n",
    "    \n",
    "    for node, data in G.nodes(data=True):\n",
    "        features = []\n",
    "        for f in feature_names:\n",
    "            value = data.get(f, 0)\n",
    "            try:\n",
    "                features.append(float(value))\n",
    "            except (ValueError, TypeError):\n",
    "                print(f\"Warning: Invalid value for {f} in node {node}: {value}. Using 0.\")\n",
    "                features.append(0.0)\n",
    "        node_features.append(features)\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    node_features_np = np.array(node_features)\n",
    "    \n",
    "    # Debug: Check feature ranges\n",
    "    for i, name in enumerate(feature_names):\n",
    "        unique_values = np.unique(node_features_np[:, i])\n",
    "        if len(unique_values) == 1:\n",
    "            print(f\"Warning: Feature '{name}' has identical values: {unique_values[0]}\")\n",
    "        else:\n",
    "            print(f\"Feature '{name}' range: {unique_values.min()} to {unique_values.max()}\")\n",
    "    \n",
    "    # Normalize only area, perimeter, eccentricity, solidity, circularity (indices 3-7)\n",
    "    scaler = StandardScaler()\n",
    "    features_to_normalize = node_features_np[:, 3:]  # Select columns 3-7\n",
    "    normalized_features = scaler.fit_transform(features_to_normalize)\n",
    "    \n",
    "    # Combine unnormalized (x, y, type) and normalized features\n",
    "    node_features_combined = np.hstack([\n",
    "        node_features_np[:, :3],  # x, y, type (unnormalized)\n",
    "        normalized_features       # area, perimeter, eccentricity, solidity, circularity (normalized)\n",
    "    ])\n",
    "    \n",
    "    # Convert to 2D tensor\n",
    "    x = torch.tensor(node_features_combined, dtype=torch.float)\n",
    "    if x.dim() != 2 or x.size(1) != len(feature_names):\n",
    "        raise ValueError(f\"Expected x to be 2D with shape [num_nodes, {len(feature_names)}], got shape {x.shape}\")\n",
    "    \n",
    "    # Extract edge indices and attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        edge_index.append([u, v])\n",
    "        weight = float(data.get('weight', 1.0))\n",
    "        edge_attr.append([weight])\n",
    "    \n",
    "    # Debug: Check edge weights\n",
    "    edge_weights = np.array(edge_attr)\n",
    "    unique_weights = np.unique(edge_weights)\n",
    "    if len(unique_weights) == 1:\n",
    "        print(f\"Warning: All edge weights are identical: {unique_weights[0]}\")\n",
    "    else:\n",
    "        print(f\"Edge weight range: {unique_weights.min()} to {unique_weights.max()}\")\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).T.contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=torch.tensor([label], dtype=torch.long),\n",
    "        original_node_indices=torch.arange(G.number_of_nodes(), dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    # Add individual node features as separate attributes\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        data[feature_name] = x[:, i]\n",
    "    \n",
    "    # Debug: Verify x tensor\n",
    "    print(f\"x tensor shape: {data.x.shape}\")\n",
    "    return data\n",
    "\n",
    "# --- Generate and Save Graphs as .pt ---\n",
    "def generate_and_save_graphs(dat_folder, graph_folder, label=0):\n",
    "    os.makedirs(graph_folder, exist_ok=True)\n",
    "    dats = natsorted(glob(os.path.join(dat_folder, \"*.dat\")))\n",
    "    \n",
    "    for dat_path in dats:\n",
    "        # Generate NetworkX graph\n",
    "        G = generate_graph_from_dat(dat_path)\n",
    "        \n",
    "        # Convert to PyG Data object\n",
    "        graph = convert_nx_to_pyg(G, label)\n",
    "        \n",
    "        # Save full graph as .pt\n",
    "        base = os.path.splitext(os.path.basename(dat_path))[0]\n",
    "        graph_path = os.path.join(graph_folder, f\"{base}.pt\")\n",
    "        torch.save(graph, graph_path)\n",
    "        print(f\"Saved full graph to {graph_path}\")\n",
    "    \n",
    "    print(f\"Generated {len(dats)} graphs in {graph_folder}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Test visualization on a sample of images\n",
    "#     subtype = 'Invasive'\n",
    "#     print(f\"Testing visualization for subtype: {subtype}\")\n",
    "    # generate_and_visualize_graphs(\n",
    "    #     image_folder=f\"./dataset/data/Photos/{subtype}/\",\n",
    "    #     dat_folder=f\"./n_detected_pannuke/{subtype}/\",\n",
    "    #     graph_folder=f\"./graphs_new_pannuke_visual/{subtype}/\",\n",
    "#         num_images=3\n",
    "#     )\n",
    "#     subtype = 'Normal'\n",
    "#     print(f\"Testing visualization for subtype: {subtype}\")\n",
    "#     generate_and_visualize_graphs(\n",
    "#         image_folder=f\"./dataset/data/Photos/{subtype}/\",\n",
    "#         dat_folder=f\"./n_detected_pannuke/{subtype}/\",\n",
    "#         graph_folder=f\"./graph_new/_pannuke_visual/{subtype}/\",\n",
    "#         num_images=3\n",
    "#     )\n",
    "#     subtype = 'InSitu'\n",
    "#     print(f\"Testing visualization for subtype: {subtype}\")\n",
    "#     generate_and_visualize_graphs(\n",
    "#         image_folder=f\"./dataset/data/Photos/{subtype}/\",\n",
    "#         dat_folder=f\"./n_detected_pannuke/{subtype}/\",\n",
    "#         graph_folder=f\"./graphs_new_pannuke_visual/{subtype}/\",\n",
    "#         num_images=3\n",
    "#     )\n",
    "#     subtype = 'Benign'\n",
    "#     print(f\"Testing visualization for subtype: {subtype}\")\n",
    "#     generate_and_visualize_graphs(\n",
    "#         image_folder=f\"./dataset/data/Photos/{subtype}/\",\n",
    "#         dat_folder=f\"./n_detected_pannuke/{subtype}/\",\n",
    "#         graph_folder=f\"./graphs_new_pannuke_visual/{subtype}/\",\n",
    "#         num_images=3\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving all graphs for subtype: ('Invasive', 3)\n",
      "3\n",
      "Saving all graphs for subtype: ('Benign', 1)\n",
      "1\n",
      "Saving all graphs for subtype: ('InSitu', 2)\n",
      "2\n",
      "Saving all graphs for subtype: ('Normal', 0)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "subtypes = {\n",
    "    'Invasive': 3,\n",
    "    'Benign': 1,\n",
    "    'InSitu': 2,\n",
    "    'Normal': 0\n",
    "}\n",
    "\n",
    "for st in subtypes.items():\n",
    "        print(f\"Saving all graphs for subtype: {st}\")\n",
    "        print(st[1])\n",
    "        # generate_and_save_graphs(\n",
    "        #     dat_folder=f\"./n_detected_pannuke/{st}/\",\n",
    "        #     graph_folder=f\"./graphs_new_pannuke/{st}/\",\n",
    "        #     label=i\n",
    "        # )\n",
    "        # i+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histographs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
