{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "root_dir = 'graphs_new_pannuke'  # top-level folder\n",
    "label_map = {'Benign': 0, 'InSitu': 1, 'Invasive': 2, 'Normal': 3}  # your subtype→label mapping\n",
    "metadata_path = 'metadata.csv'\n",
    "\n",
    "with open(metadata_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['graph_path', 'label'])\n",
    "\n",
    "    for subtype in os.listdir(root_dir):\n",
    "        subtype_path = os.path.join(root_dir, subtype)\n",
    "        if not os.path.isdir(subtype_path):\n",
    "            continue\n",
    "\n",
    "        label = label_map.get(subtype, -1)\n",
    "        if label == -1:\n",
    "            print(f\"Unknown label for {subtype}\")\n",
    "            continue\n",
    "\n",
    "        for fname in os.listdir(subtype_path):\n",
    "            if fname.endswith('.graphml'):\n",
    "                rel_path = os.path.join(subtype, fname)\n",
    "                writer.writerow([os.path.join(root_dir, rel_path), label])\n",
    "\n",
    "print(f\"Metadata written to {metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from glob import glob\n",
    "from natsort import natsorted\n",
    "import pandas as pd\n",
    "\n",
    "# --- Create Subgraphs ---\n",
    "def create_subgraphs(graph, window_size, step_size):\n",
    "    subgraphs = []\n",
    "    num_nodes = graph.num_nodes\n",
    "\n",
    "    # Verify x tensor is 2D\n",
    "    if graph.x.dim() != 2:\n",
    "        raise ValueError(f\"Expected graph.x to be 2D, got shape {graph.x.shape}\")\n",
    "\n",
    "    # Extract 'type' feature (3rd column, index 2)\n",
    "    nucleus_types = graph.x[:, 2]  # 'type' is at index 2\n",
    "    type1_nodes = (nucleus_types == 1).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    for center in type1_nodes.tolist()[::step_size]:\n",
    "        # Define window centered at the type-1 node\n",
    "        start = max(0, center - window_size // 2)\n",
    "        end = start + window_size\n",
    "\n",
    "        # Correct window if it exceeds graph bounds\n",
    "        if end > num_nodes:\n",
    "            end = num_nodes\n",
    "            start = max(0, end - window_size)\n",
    "\n",
    "        # Build node index list and remapping dictionary\n",
    "        node_indices = list(range(start, end))\n",
    "        id_map = {old: i for i, old in enumerate(node_indices)}\n",
    "\n",
    "        # Filter and remap edges\n",
    "        mask = torch.tensor([\n",
    "            (src.item() in id_map and dst.item() in id_map)\n",
    "            for src, dst in graph.edge_index.T\n",
    "        ], dtype=torch.bool)\n",
    "        edge_index = graph.edge_index[:, mask]\n",
    "        edge_attr = graph.edge_attr[mask] if graph.edge_attr is not None else None\n",
    "\n",
    "        # Remap edge indices\n",
    "        edge_index = torch.tensor([\n",
    "            [id_map[src.item()], id_map[dst.item()]]\n",
    "            for src, dst in edge_index.T\n",
    "        ], dtype=torch.long).T\n",
    "\n",
    "        # Fallback if no edges remain (self-loops to avoid empty graphs)\n",
    "        if edge_index.size(1) == 0:\n",
    "            edge_index = torch.stack([\n",
    "                torch.arange(len(node_indices)),\n",
    "                torch.arange(len(node_indices))\n",
    "            ], dim=0)\n",
    "            edge_attr = torch.ones((len(node_indices), 1), dtype=torch.float)  # Default weight=1 for self-loops\n",
    "\n",
    "        # Create the subgraph\n",
    "        subgraph = Data(\n",
    "            x=graph.x[start:end],\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=graph.y,\n",
    "            original_node_indices=torch.tensor(node_indices, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "        # Add individual node features as separate attributes\n",
    "        feature_names = ['x', 'y', 'type', 'area', 'perimeter', 'eccentricity', 'solidity', 'circularity']\n",
    "        for i, feature_name in enumerate(feature_names):\n",
    "            subgraph[feature_name] = subgraph.x[:, i]\n",
    "\n",
    "        subgraphs.append(subgraph)\n",
    "\n",
    "    return subgraphs\n",
    "\n",
    "# --- Generate and Save Subgraphs as .pt ---\n",
    "def generate_and_save_subgraphs(graph_folder, subgraph_folder, window_size=62, step_size=1):\n",
    "    os.makedirs(subgraph_folder, exist_ok=True)\n",
    "    graphs = natsorted(glob(os.path.join(graph_folder, \"*.pt\")))\n",
    "    \n",
    "    metadata = []\n",
    "    \n",
    "    for graph_path in graphs:\n",
    "        # Load full graph\n",
    "        graph = torch.load(graph_path, weights_only=False)\n",
    "        \n",
    "        # Generate subgraphs\n",
    "        subgraphs = create_subgraphs(graph, window_size=window_size, step_size=step_size)\n",
    "        \n",
    "        # Save subgraphs as .pt\n",
    "        base = os.path.splitext(os.path.basename(graph_path))[0]\n",
    "        label = graph.y.item()\n",
    "        for i, sg in enumerate(subgraphs):\n",
    "            sg_filename = f\"{base}_sg{i}.pt\"\n",
    "            sg_path = os.path.join(subgraph_folder, sg_filename)\n",
    "            torch.save(sg, sg_path)\n",
    "            print(f\"Saved subgraph to {sg_path}\")\n",
    "            metadata.append({'subgraph_path': sg_path, 'label': label})\n",
    "        \n",
    "        print(f\"{graph_path}: {len(subgraphs)} subgraphs generated\")\n",
    "    \n",
    "    # Save metadata CSV\n",
    "    metadata_csv = os.path.join(subgraph_folder, 'sub_gphmeta.csv')\n",
    "    pd.DataFrame(metadata).to_csv(metadata_csv, index=False)\n",
    "    print(f\"Metadata saved to {metadata_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import os\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# --- Convert .graphml → PyG Data object ---\n",
    "def convert_nx_to_pyg(graphml_path, label):\n",
    "    # Read graphml file\n",
    "    G = nx.read_graphml(graphml_path)\n",
    "    \n",
    "    # Convert node labels to integers\n",
    "    G = nx.convert_node_labels_to_integers(G)\n",
    "    \n",
    "    # Extract node features\n",
    "    node_features = []\n",
    "    feature_names = ['x', 'y', 'type', 'area', 'perimeter', 'eccentricity', 'solidity', 'circularity']\n",
    "    \n",
    "    # Debug: Print raw node attributes to verify values\n",
    "    print(f\"Processing graph: {graphml_path}\")\n",
    "    for node, data in G.nodes(data=True):\n",
    "        features = []\n",
    "        for f in feature_names:\n",
    "            value = data.get(f, 0)\n",
    "            try:\n",
    "                features.append(float(value))\n",
    "            except (ValueError, TypeError):\n",
    "                print(f\"Warning: Invalid value for {f} in node {node}: {value}. Using 0.\")\n",
    "                features.append(0.0)\n",
    "        node_features.append(features)\n",
    "    \n",
    "    # Convert to numpy array for analysis\n",
    "    node_features_np = np.array(node_features)\n",
    "    \n",
    "    # Debug: Check if features have identical values\n",
    "    for i, name in enumerate(feature_names):\n",
    "        unique_values = np.unique(node_features_np[:, i])\n",
    "        if len(unique_values) == 1:\n",
    "            print(f\"Warning: Feature '{name}' has identical values: {unique_values[0]}\")\n",
    "        else:\n",
    "            print(f\"Feature '{name}' range: {unique_values.min()} to {unique_values.max()}\")\n",
    "    \n",
    "    # Optional: Normalize node features\n",
    "    scaler = StandardScaler()\n",
    "    node_features_normalized = scaler.fit_transform(node_features_np)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    x = torch.tensor(node_features_normalized, dtype=torch.float)\n",
    "    \n",
    "    # Extract edge indices and attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        edge_index.append([u, v])\n",
    "        weight = float(data.get('weight', 1.0))\n",
    "        edge_attr.append([weight])\n",
    "    \n",
    "    # Debug: Check edge weights\n",
    "    edge_weights = np.array(edge_attr)\n",
    "    unique_weights = np.unique(edge_weights)\n",
    "    if len(unique_weights) == 1:\n",
    "        print(f\"Warning: All edge weights are identical: {unique_weights[0]}\")\n",
    "    else:\n",
    "        print(f\"Edge weight range: {unique_weights.min()} to {unique_weights.max()}\")\n",
    "    \n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).T.contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    \n",
    "    # Create PyG Data object\n",
    "    data = Data(\n",
    "        x=x,\n",
    "        edge_index=edge_index,\n",
    "        edge_attr=edge_attr,\n",
    "        y=torch.tensor([label], dtype=torch.long),\n",
    "        original_node_indices=torch.arange(G.number_of_nodes(), dtype=torch.long)\n",
    "    )\n",
    "    \n",
    "    # Add individual node features as separate attributes\n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        data[feature_name] = x[:, i]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# --- Create Subgraphs ---\n",
    "def create_subgraphs(graph, window_size, step_size):\n",
    "    subgraphs = []\n",
    "    num_nodes = graph.num_nodes\n",
    "\n",
    "    # Extract 'type' feature (assuming it's the 3rd column, index 2)\n",
    "    nucleus_types = graph.x[:, 2]  # 'type' is at index 2\n",
    "    type1_nodes = (nucleus_types == 1).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    for center in type1_nodes.tolist()[::step_size]:\n",
    "        # Define window centered at the type-1 node\n",
    "        start = max(0, center - window_size // 2)\n",
    "        end = start + window_size\n",
    "\n",
    "        # Correct window if it exceeds graph bounds\n",
    "        if end > num_nodes:\n",
    "            end = num_nodes\n",
    "            start = max(0, end - window_size)\n",
    "\n",
    "        # Build node index list and remapping dictionary\n",
    "        node_indices = list(range(start, end))\n",
    "        id_map = {old: i for i, old in enumerate(node_indices)}\n",
    "\n",
    "        # Filter and remap edges\n",
    "        mask = torch.tensor([\n",
    "            (src.item() in id_map and dst.item() in id_map)\n",
    "            for src, dst in graph.edge_index.T\n",
    "        ], dtype=torch.bool)\n",
    "        edge_index = graph.edge_index[:, mask]\n",
    "        edge_attr = graph.edge_attr[mask] if graph.edge_attr is not None else None\n",
    "\n",
    "        # Remap edge indices\n",
    "        edge_index = torch.tensor([\n",
    "            [id_map[src.item()], id_map[dst.item()]]\n",
    "            for src, dst in edge_index.T\n",
    "        ], dtype=torch.long).T\n",
    "\n",
    "        # Fallback if no edges remain (self-loops to avoid empty graphs)\n",
    "        if edge_index.size(1) == 0:\n",
    "            edge_index = torch.stack([\n",
    "                torch.arange(len(node_indices)),\n",
    "                torch.arange(len(node_indices))\n",
    "            ], dim=0)\n",
    "            edge_attr = torch.ones((len(node_indices), 1), dtype=torch.float)  # Default weight=1 for self-loops\n",
    "\n",
    "        # Create the subgraph\n",
    "        subgraph = Data(\n",
    "            x=graph.x[start:end],\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=graph.y,\n",
    "            original_node_indices=torch.tensor(node_indices, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "        # Add individual node features as separate attributes\n",
    "        feature_names = ['x', 'y', 'type', 'area', 'perimeter', 'eccentricity', 'solidity', 'circularity']\n",
    "        for i, feature_name in enumerate(feature_names):\n",
    "            subgraph[feature_name] = subgraph.x[:, i]\n",
    "\n",
    "        subgraphs.append(subgraph)\n",
    "\n",
    "    return subgraphs\n",
    "\n",
    "# --- PyG InMemoryDataset from metadata.csv ---\n",
    "class SubgraphDatasetFromMetadata(InMemoryDataset):\n",
    "    def __init__(self, metadata_path, transform=None, pre_transform=None, out_dir='./subgraphs', out_csv='subghmeta.csv'):\n",
    "        super(SubgraphDatasetFromMetadata, self).__init__('.', transform, pre_transform)\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "        self.data_list = []\n",
    "        self.records = []\n",
    "\n",
    "        for _, row in metadata.iterrows():\n",
    "            graph_path = row['graph_path']\n",
    "            label = int(row['label'])\n",
    "            graph_name = os.path.splitext(os.path.basename(graph_path))[0]\n",
    "\n",
    "            graph = convert_nx_to_pyg(graph_path, label)\n",
    "            subgraphs = create_subgraphs(graph, window_size=62, step_size=1)\n",
    "            for i, sg in enumerate(subgraphs):\n",
    "                sg_filename = f\"{graph_name}_sg{i}.pt\"\n",
    "                sg_path = os.path.join(out_dir, sg_filename)\n",
    "                torch.save(sg, sg_path)\n",
    "                self.records.append({'subgraph_path': sg_path, 'label': label})\n",
    "                self.data_list.append(sg)\n",
    "            print(f\"{row['graph_path']}: {len(subgraphs)} subgraphs generated\")\n",
    "\n",
    "        self.data, self.slices = self.collate(self.data_list)\n",
    "\n",
    "        # Save metadata CSV\n",
    "        pd.DataFrame(self.records).to_csv(out_csv, index=False)\n",
    "        print(f\"Subgraphs saved to '{out_dir}', metadata saved to '{out_csv}'\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [data.y.item() for data in self.data_list]\n",
    "\n",
    "# --- Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Example with a single graph\n",
    "    graphml_path = 'graphs_new_pannuke/Benign/b017.graphml'\n",
    "    label = 0  # Example label (e.g., 0 for Benign)\n",
    "    \n",
    "    # Convert and debug full graph\n",
    "    full_graph = convert_nx_to_pyg(graphml_path, label)\n",
    "    print(\"Original Full Graph:\")\n",
    "    print(full_graph)\n",
    "    \n",
    "    # Create and print an example subgraph\n",
    "    subgraphs = create_subgraphs(full_graph, window_size=62, step_size=1)\n",
    "    if subgraphs:\n",
    "        print(\"\\nExample SubGraph:\")\n",
    "        print(subgraphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_graphml('graphs_new_pannuke/Benign/b017.graphml')\n",
    "for node, data in G.nodes(data=True):\n",
    "    print(f\"Node {node}: {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = SubgraphDatasetFromMetadata(\n",
    "    metadata_path='metadata.csv',\n",
    "    out_dir='./subgraphs_pannuke_s20',\n",
    "    out_csv='sub_gphmeta_pannuke.csv'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sb_meta = pd.read_csv('sub_gphmeta.csv')\n",
    "# Split into train/test\n",
    "train_meta, test_meta = train_test_split(sb_meta, test_size=0.2, stratify=sb_meta[\"label\"], random_state=42)\n",
    "\n",
    "# Save new metadata files\n",
    "train_meta.to_csv(\"train_meta.csv\", index=False)\n",
    "test_meta.to_csv(\"test_meta.csv\", index=False)\n",
    "\n",
    "print(f\"Train size: {len(train_meta)}, Test size: {len(test_meta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of graphs\n",
    "num_graphs = len(dataset)\n",
    "\n",
    "# Number of classes (unique labels)\n",
    "num_classes = dataset.num_classes\n",
    "\n",
    "# Get the labels (for each graph in the dataset)\n",
    "labels = [data.y.item() for data in dataset]\n",
    "\n",
    "# Calculate the average number of nodes and edges\n",
    "total_nodes = sum(data.num_nodes for data in dataset)\n",
    "total_edges = sum(data.num_edges for data in dataset)\n",
    "avg_nodes = total_nodes / num_graphs\n",
    "avg_edges = total_edges / num_graphs\n",
    "\n",
    "# Display the results\n",
    "print(f\"Number of graphs: {num_graphs}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Labels: {set(labels)}\")\n",
    "print(f\"Average number of nodes: {avg_nodes}\")\n",
    "print(f\"Average number of edges: {avg_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads=8, dropout=0.6):\n",
    "        super(GAT, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "        \n",
    "        self.conv1 = GATConv(dataset.num_node_features, hidden_channels, heads=heads, dropout=dropout)\n",
    "        # Combine the heads by averaging\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False, dropout=dropout)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, dataset.num_classes)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # First GAT layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GAT layer\n",
    "        x, attn_weights = self.conv2(x, edge_index, return_attention_weights=True)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Global mean pooling\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "\n",
    "        # Classifier\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1), attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "hidden_channels = 64\n",
    "heads = 16\n",
    "dropout = 0.6\n",
    "model = GAT(hidden_channels=hidden_channels, heads=heads, dropout=dropout)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Move data to device\n",
    "def move_to_device(batch, device):\n",
    "    batch = batch.to(device)\n",
    "    return batch\n",
    "\n",
    "# Weakly supervised training\n",
    "model.train()\n",
    "for epoch in range(50):  # Number of epochs\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch = next(iter(train_loader))\n",
    "\n",
    "    print(\"Type of batch:\", type(batch))\n",
    "    print(\"Batch object keys:\", batch.keys)\n",
    "    print(\"Node feature shape (x):\", batch.x.shape)\n",
    "    print(\"Edge index shape:\", batch.edge_index.shape)\n",
    "    print(\"Labels (y):\", batch.y)\n",
    "    print(\"Batch vector shape (batch):\", batch.batch.shape)  # tells which node belongs to which graph\n",
    "\n",
    "    for batch in train_loader:\n",
    "        batch = move_to_device(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "        out, attn_weights = model(batch)\n",
    "        loss = criterion(out, batch.y)  # Compute loss using graph labels\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        _, predicted = torch.max(out, dim=1)\n",
    "        correct += (predicted == batch.y).sum().item()\n",
    "        total += batch.y.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type([0]))\n",
    "print(\"Type of train_loader:\", type(train_loader))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histographs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
